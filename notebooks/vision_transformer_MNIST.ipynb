{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff0c42c",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) for MNIST\n",
    "\n",
    "In this notebook, we test a ViT implementation with MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e651b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# vit-pytorch\n",
    "from vit_pytorch import ViT\n",
    "#\n",
    "import numpy as np\n",
    "#\n",
    "import tqdm\n",
    "import time\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988c231",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99a591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "DOWNLOAD_PATH = \"./data/mnist\"\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 1000\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "# transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "#                                             torchvision.transforms.Normalize([0.1307,], [0.3081,])])\n",
    "\n",
    "# Dataset\n",
    "train_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=True, download=True,\n",
    "                                       transform=transform)\n",
    "test_set  = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=False, download=True,\n",
    "                                       transform=transform)\n",
    "# Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_set,  batch_size=BATCH_SIZE_TEST,  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85bf48d",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b19bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning functino for one epoch\n",
    "def train_epoch(model, optimizer, loss_fn, data_loader, loss_history):\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    \n",
    "    model.train() # Enter the training mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for _i, (data, target) in enumerate(tqdm.tqdm(data_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if _i % 100 == 0:\n",
    "#             print('[' +  '{:5}'.format(_i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
    "#                   ' (' + '{:3.0f}'.format(100 * _i / len(data_loader)) + '%)]  Loss: ' +\n",
    "#                   '{:6.4f}'.format(loss.item()))\n",
    "            loss_history.append(loss.item())\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    print('\\nAverage train loss: ' + '{:.4f}'.format(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52236099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, loss_fn, data_loader, loss_history):\n",
    "    model.eval() # Enter the evaluation mode\n",
    "    \n",
    "    total_samples = len(data_loader.dataset)\n",
    "    correct_samples = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm.tqdm(data_loader):\n",
    "            output = model(data)\n",
    "            # Analyze the result\n",
    "            loss = loss_fn(output, target) # The loss\n",
    "            _, pred = torch.max(output, dim=1) # The predicted class\n",
    "            # Accumulate\n",
    "            total_loss += loss.item()\n",
    "            correct_samples += pred.eq(target).sum()\n",
    "            \n",
    "    avg_loss = total_loss / total_samples\n",
    "    loss_history.append(avg_loss)\n",
    "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
    "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
    "          '{:5}'.format(total_samples) + ' (' +\n",
    "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb33b0",
   "metadata": {},
   "source": [
    "## Generate the Model (ViT) and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8449e08",
   "metadata": {},
   "source": [
    "A ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed113fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate a model\n",
    "# model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
    "#             dim=64, depth=6, heads=8, mlp_dim=128)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e28e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "model_vit = ViT(image_size=28, patch_size=4, num_classes=10, channels=1,\n",
    "            dim=22, depth=3, heads=1, mlp_dim=22)\n",
    "optimizer_vit = torch.optim.Adam(model_vit.parameters(), lr=0.003)\n",
    "loss_fn_vit = torch.nn.CrossEntropyLoss()\n",
    "#\n",
    "# print(model_vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134543dd",
   "metadata": {},
   "source": [
    "A simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd20b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class CNNnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        # return F.log_softmax(x)\n",
    "        return x # directly output the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2abba4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "model_cnn = CNNnet()\n",
    "optimizer_cnn = torch.optim.Adam(model_cnn.parameters(), lr=0.003)\n",
    "loss_fn_cnn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb69b2",
   "metadata": {},
   "source": [
    "## Model Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251bd532",
   "metadata": {},
   "source": [
    "The tool for measuring the model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bfb884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    # The total size\n",
    "    total_size = param_size + buffer_size\n",
    "    # The scaling for display\n",
    "    if total_size >= 1024**2:\n",
    "        size_level = 'MB'\n",
    "        size_scale = 1.0/1024**2\n",
    "    else:\n",
    "        size_level = 'kB'\n",
    "        size_scale = 1.0/1024\n",
    "    print('='*30)\n",
    "    print(f'model paramater size: {(param_size*size_scale) :.3f}{size_level}')\n",
    "    print(f'model buffer size: {(buffer_size*size_scale) :.3f}{size_level}')\n",
    "    print('-'*30)\n",
    "    print(f'model buffer size: {(total_size*size_scale) :.3f}{size_level}')\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c98de0",
   "metadata": {},
   "source": [
    "Check the model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc899bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "model paramater size: 86.062kB\n",
      "model buffer size: 0.000kB\n",
      "------------------------------\n",
      "model buffer size: 86.062kB\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "get_model_size(model_vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "386ea2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "model paramater size: 85.312kB\n",
      "model buffer size: 0.000kB\n",
      "------------------------------\n",
      "model buffer size: 85.312kB\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "get_model_size(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215c326",
   "metadata": {},
   "source": [
    "## Train the Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8657e5bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, loss_fn, train_loader, test_loader, n_epochs=10):\n",
    "    start_time = time.time()\n",
    "    train_loss_history, test_loss_history = list(), list()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        print('EPOCH:', epoch)\n",
    "        train_epoch(model, optimizer, loss_fn, train_loader, train_loss_history)\n",
    "        evaluate(model, loss_fn, test_loader, test_loss_history)\n",
    "\n",
    "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46a387",
   "metadata": {},
   "source": [
    "The ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e38edb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [01:42<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train loss: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average test loss: 0.0003  Accuracy: 8942/10000 (89.42%)\n",
      "\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [01:37<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train loss: 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average test loss: 0.0002  Accuracy: 9325/10000 (93.25%)\n",
      "\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████▎                         | 407/600 [00:58<00:27,  6.93it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_vit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_vit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_vit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, optimizer, loss_fn, train_loader, test_loader, n_epochs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH:\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     evaluate(model, loss_fn, test_loader, test_loss_history)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecution time:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:5.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, loss_fn, data_loader, loss_history)\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# Enter the training mode\u001b[39;00m\n\u001b[0;32m      7\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _i, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm\u001b[38;5;241m.\u001b[39mtqdm(data_loader)):\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\functional.py:151\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    150\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 151\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbands\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    153\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(model_vit, optimizer_vit, loss_fn_vit, train_loader, test_loader, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8afe1",
   "metadata": {},
   "source": [
    "The CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(model_cnn, optimizer_cnn, loss_fn_cnn, train_loader, test_loader, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21564be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f04869f",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0603333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, data_batch):\n",
    "    # Prediction by model, in batch\n",
    "    model.eval()\n",
    "    pred_scores = model(data_batch)\n",
    "    _, preds = torch.max(pred_scores, dim=1) # The predicted class\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6086bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_tiles(images, preds, labels):\n",
    "    # plot the first 20 images in the batch, along with the corresponding labels and predictions\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    for idx in np.arange(20):\n",
    "        ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "        plt.imshow(np.transpose(images[idx], (1, 2, 0))) # (c,h,w) --> (h,w,c)\n",
    "        ax.set_title(f'p:{preds[idx]}' + (f' gt:{labels[idx]} (x)' if preds[idx]!=labels[idx] else ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9630686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training data\n",
    "# obtain one batch of training images\n",
    "dataiter = iter( train_loader )\n",
    "images_tensor, labels = dataiter.next()\n",
    "images = images_tensor.numpy() # convert images to numpy for display\n",
    "\n",
    "print(f'\\nimages[0,0,0,0] = {images[0,0,0,0]}\\n')\n",
    "print(f'\\nimages[0].mean() = {images[0].mean()}\\n')\n",
    "\n",
    "# Prediction by model, in batch\n",
    "preds_vit = get_prediction(model_vit, images_tensor)\n",
    "preds_cnn = get_prediction(model_cnn, images_tensor)\n",
    "\n",
    "print(f'{preds_vit = }')\n",
    "print(f'{preds_cnn = }')\n",
    "\n",
    "\n",
    "# plot the first 20 images in the batch, along with the corresponding labels and predictions\n",
    "# fig = plt.figure(figsize=(25, 4))\n",
    "# for idx in np.arange(20):\n",
    "#     ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "#     plt.imshow(np.transpose(images[idx], (1, 2, 0))) # (c,h,w) --> (h,w,c)\n",
    "#     ax.set_title(f'p:{preds[idx]}' + (f' gt:{labels[idx]} (x)' if preds[idx]!=labels[idx] else ''))\n",
    "plot_image_tiles(images, preds_vit, labels)\n",
    "plot_image_tiles(images, preds_cnn, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce14c47a",
   "metadata": {},
   "source": [
    "## Calculate the Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e3210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
